ご提示いただいたテキストを、より読みやすく、技術用語が際立つようにマークダウン形式に整えました。そのままコピーしてお使いいただけます。

---

## エンジニア向け：実装における3つの重要ポイント

### 1. IssueOpsによるパラメータの安全な受け渡し

GitHub Pagesからは直接GitHub Actionsを起動させるのではなく、GitHub API経由でIssueを作成させます。そのIssueの本文に `START_ROW` や `END_ROW` を記載し、Actions内で `actions/github-script` を使ってパース（抽出）することで、安全かつ動的にパラメータをPythonへ渡せます。

### 2. `if: always()` を活用した途中データの保護

スクレイピング処理を行う `Run Scraping Script` の次のステップに `if: always()` を設定しています。これにより、万が一処理が6時間のタイムアウトに達して強制終了されたり、途中でエラーが起きたりした場合でも、**それまでに `output_data/` フォルダへ書き出されたデータは必ずGitHub上に保存（アップロード）**されます。

### 3. Pythonスクリプト側の実装要件（ご留意事項）

このWorkflowを最大限活かすため、実行されるPythonスクリプト（`src/scraper.py`）は以下のように設計しておくことを推奨します。

* 受け取った `START_ROW` から `END_ROW` までの処理をループで行う。
* メモリ不足やタイムアウトに備え、データを100件〜500件など一定数取得するごとに `output_data/` 配下にCSVやJSONとしてこまめにファイル出力する。
* 次回再実行時に、出力済みのファイルを読み込んで「どこまで終わったか」を判定し、続きから処理を開始できる（レジューム機能）ようにする。

---

上記3点目（ご留意事項）に記載されている「途中再開（レジューム機能）」や「こまめなファイル出力」を実装した、**Python側（`src/scraper.py`）の具体的なサンプルコード**も併せて作成しましょうか？
